---
title: "Replication of 'Visual sense of number vs. sense of magnitude in humans and machines' by Testolin, A., Dolfi, S., Rochus, M. et al. (2020, Sci Rep)"
author: "Wenqing Cao (cwenqing@ucsd.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 3
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

Link to github folder: https://github.com/psyc-201/testolin2020
Link to paradigm: https://psyc-201.github.io/testolin2020/testolin2020_experiment/experiment

## Introduction

Justification: 

It is widely believed that the approximate number system (ANS) supports formation of non-exact representation of quantities in adults, children, and even animals (Dehaene, 2011). ANS affords abilities like quickly discriminating two different numerical quantities or estimating the amount of items. The acuity of discrimintation is proportional to the ratio of the two quantities, and is found to predict future math learning performance. However, debate exists around whether numerosity is the main perceptual feature driving ANS, or numerosity is actually estimated from non-numerical visual features. This paper attempted to reconcile this debate by testing deep neural networks and human participants in the same task, adopting a stimulus space that could disentangle the contribution of numerical and non-numerical features. For this replication project, I will try to replicate the human experiment in this paper. It is related to my side research interest of probing ANS in vision language models. 

Methods: 

The research question is whether visual numerosity is a primary perceptual attribute or whether people estimate numerosity from continuous magnitudes. To test this, researchers adopted the dot array comparison task, where the participant sees two side-by-side dot arrays for a limited period of time, and then needs to indicate which array has a higher *number* of dots. To quantitatively estimate the contribution of non-numerical features, researchers constructed a 3-D orthogonal stimulus space which includes 3 dimensions: numerosity, size, and spacing. Specifically, they generated a database of 21970 images of dot arrays by picking 13 levels on each dimension, each level put evenly on a log scale. They then randomly sampled 300 pairs of dot images from the database as test stimuli, encompassing ratios from 0.5 to 0.9, biasing the harder ratios. Participants indicated which image in the pair had more dots by pressing left or right arrows on keyboard. 

They recruited volunteer college students (n = 40) as huamn subjects. Study session length for each participant ~ 30 minutes. The power of their human behavioral finding was strong, mean accuracy = 83%, with significant individual-level GLM fits (R² = 0.55, chi-square value = 191.14, p < 0.001). Coefficient fits for each dimension were significant for numerosity (t(39)=23.54, p<.001) and spacing (t(39)=7.21, p<.001), but not for size (t(39)=1.37, p=.18). 

The key analysis that I'm trying to replicate is the GLM fit on individual level, and the coefficient fit for each dimension. I will use the original stimuli (the 300 sampled pairs). I could recruit less participants (n = 25, 300 trials/30 mins each)   

What’s the measure of interest?
- Accuracy on a dot array numerosity stroop task 

What construct does it map to?
- Human adults’ approximate representation of numerosity

Is its estimate reported?
- Taking mean
- Effect size R^2 = 0.55
- Unit: Number of correct trials? 

Any measure of variability?
- SE? 


## Methods

### Power Analysis

Original effect size, power analysis for samples to achieve 80%, 90%, 95% power to detect that effect size.  Considerations of feasibility for selecting planned sample size.

A: The original study fitted a GLM model for each individaul, including 3 regressors numerosity, size, and spacing. The numerosity coefficient had the largest effect (t(39) = 23.54, p < 0.001); spacing coefficient also showed a significant effect (t(39) = 7.21, p < 0.001); size coefficient did not reveal a significant effect (t(39) = 1.37, p < 0.18). I calculated effect size by t/sqrt(n). Effect sizes for the numerosity, spacing, and sizing dimensions are 3.72 (very large), 1.14 (large), and 0.22 (small), respectively. Observed powers are 100% (numerosity), 100% (spacing), 24% (size), respectively. 

For numerosity dimension, 3, 4, 5 participants are needed to achieve 80%, 90%, 95% power. For spacing dimension, 8, 10, 13 participants are needed to achieve 80%, 90%, 95% power. For size dimension, 163, 219, 274 participants are needed to achieve 80%, 90%, 95% power. 

### Planned Sample

Planned sample size and/or termination rule, sampling frame, known demographics if any, preselection rules if any.

A: I plan to collect 30 samples, overpowered to account for a potentially high exclusion rate due to poor data quality or unfinished, given that the study is relatively long and tiring. Participants will be college undergraduate students in the local San Diego area, such that the sample's mean age and education level will roughly match the original study sample which consisted of 40 volunteer students (mean age = 23.7 years, range = 20-28, females = 80%). 

### Materials

All materials - can quote directly from original article - just put the text in quotations and note that this was followed precisely.  Or, quote directly and just point out exceptions to what was described in the original article.

A: "Images of size 200 × 200 pixels were generated by randomly placing white dots on a black background. For the discrimination task there were 13 levels of Numerosity (range 7–28), 13 levels of Size (range  2.6–10.4 pixels × 105) and 13 levels of Spacing (range 80–320 pixels × 105), evenly spaced on a logarithmic scale. For each selected point in the stimulus space 10 different images were generated by randomly varying dots displacement, resulting in a dataset of 21970 unique images. For the human experiment we randomly selected images from the dataset to create 300 image pairs with different magnitude ratios, oversampling the more difficult numerosity ratios (10% with ratio between 0.5 and 0.6; 20% with ratio between 0.6 and 0.7; 30% with ratio between 0.7 and 0.8; 40% with ratio between 0.8 and 0.9)." 

Because the authors did not publish code or data specifying the 300 image pairs used in the study, I will produce my own code to randomly select the 300 image pairs. The authors did publish the exact dimensions and point coordinates for all 21970 unique dot array images, so everything else will be equal.

### Procedure	

Can quote directly from original article - just put the text in quotations and note that this was followed precisely.  Or, quote directly and just point out exceptions to what was described in the original article.

A: "Stimuli were projected on a 19-inch color screen. Participants sat approximately 70 cm from the screen and placed their head on a chin rest. Participants were verbally instructed to select the stimulus with more dots, responding with the left and right arrows of the keyboard depending on its side of appearance (feedback was given only during few practice trials). The task consisted in 3 blocks of 100 trials each, for a total of 300 trials. Each trial began with a fixation cross at the center of the screen (500 ms), followed by the simultaneous presentation of two stimuli (250 ms), one at the right and one at the left of the cross with eccentricity of ~12 visual degrees, and then by two masks of black and white Gaussian noise in the same positions (150 ms). A black screen was then displayed until response, without time limit. After response, a pseudorandom inter-trial interval between 1250 and 1750 ms occurred."

### Analysis Plan

Can also quote directly, though it is less often spelled out effectively for an analysis strategy section.  The key is to report an analysis strategy that is as close to the original - data cleaning rules, data exclusion rules, covariates, etc. - as possible.  

**Clarify key analysis of interest here**  You can also pre-specify additional analyses you plan to do.

A: "All responses below stimulus presentation time were considered outliers, as well as response times over two standard deviations from the participant’s mean response time in equally difficult trials (based on numerosity ratio). A generalized linear model (with probit link function) was then fitted to the choice data of each  participant41, which was modeled as a function of the three regressors Numerosity, Size and Spacing" 

Instead of running a generalized linear model for each individual, I will fit a group-level general linear mixed effect model to all trials across participants to investigate the effect of numerosity, size, spacing, and random effect for each individual. The dependent measure will be the probability of getting a particular trial correct. A GLMM is suitable because participants' responses were binary choices (left/right array). 

### Differences from Original Study

Explicitly describe known differences in sample, setting, procedure, and analysis plan from original study.  The goal, of course, is to minimize those differences, but differences will inevitably occur.  Also, note whether such differences are anticipated to make a difference based on claims in the original article or subsequent published research on the conditions for obtaining the effect.

A: In my replication project, the sample size (n = 25) will be smaller than the previous study (n = 40). The setting and procedure will be the same. There will be a slight difference in stimuli as I will be randomly sampling the 300 dot image pairs from their stimuli database. The analysis plan will be difference as I will run a GLMM on group level including random intercepts, instead of running a GLM with each individual participant. 

### Methods Addendum (Post Data Collection)

You can comment this section out prior to final report with data collection.

#### Actual Sample
  Sample size, demographics, data exclusions based on rules spelled out in analysis plan

#### Differences from pre-data collection methods plan
  Any differences from what was described as the original plan, or “none”.


## Results


### Data preparation

Data preparation following the analysis plan.

A: First, based on the exclusion criteria established, trials where a response is made below stimulus presentation time or where response time is over 2*SD from the participants' mean response time in equally difficult trials will be eliminated from the analysis. To achieve this, I will first merge all participants' data into a dataframe. Then I will apply a filter that only selects data rows where response time > stimulus presentation time (250 ms). I will also apply a filter that removes data where response time > mean(response time) + 2SD. I will make sure that columns include individual participant number, response time, 1/0 if correct on the trial, ratio in numerosity, ratio in size, ratio in spacing. 
	
```{r include=F}
### Data Preparation

#### Load Relevant Libraries and Functions
library(tidyverse)
library(broom)

#### Import data

select <- dplyr::select
filter <- dplyr::filter

metadata <- read_csv("/Users/annacao/Documents/github/testolin2020/testolin2020_experiment/paired_images/pairs_metadata.csv") %>%
    select(
      pair_num,
      pair_order,
      pair_ratio,
      left_image_id = image_id,
      left_numerosity = numerosity,
      left_TSA = TSA,
      left_FA = FA,
      left_CH = CH,
      left_cumArea = cumArea,
      left_role = role,   # anchor or partner
      position,   # which side this image is on
      greater_role,    # in this pair which role has more dots
      greater_side     # in this pair which side has more dots
    )

left_stim <- metadata %>%
    filter(position == "left") %>%
    select(-position)

right_stim <- metadata %>%
    filter(position == "right") %>%
    select(
      pair_order,
      right_image_id = left_image_id,
      right_numerosity = left_numerosity,
      right_TSA = left_TSA,
      right_FA = left_FA,
      right_CH = left_CH,
      right_cumArea = left_cumArea,
      right_role = left_role
    )

pairs_complete <- left_stim %>%
    left_join(right_stim, by = "pair_order") %>%
    select(
      pair_order,
      pair_ratio,
      left_image_id, left_numerosity, left_TSA, left_FA, left_CH, left_cumArea, left_role,
      right_image_id, right_numerosity, right_TSA, right_FA, right_CH, right_cumArea, right_role,
      greater_role, greater_side
    )

pairs_complete <- pairs_complete %>% 
  mutate(
    left_Size    = (left_TSA  / left_numerosity)^2,
    right_Size   = (right_TSA / right_numerosity)^2,
    left_Spacing = (left_FA   / left_numerosity)^2,
    right_Spacing= (right_FA  / right_numerosity)^2,
  )

pairs_complete <- pairs_complete %>% 
  mutate(
    log_num_ratio = log2(right_numerosity) - log2(left_numerosity),
    log_size_ratio = log2(right_Size) - log2(left_Size),
    log_spacing_ratio = log2(right_Spacing) - log2(left_Spacing)
  )

pairs_complete <- pairs_complete %>% 
  mutate(
    congruent_spacing = sign(log_num_ratio) == sign(log_spacing_ratio),
    congruent_size    = sign(log_num_ratio) == sign(log_size_ratio)
  )


# load participant data
load_participant_data <- function(participant_path) {
  participant_data <- read_csv(participant_path) %>%
    filter(task == "numerosity_discrimination") %>%
    select(
      pair_order,
      pair_num,
      pair_ratio,
      rt,
      response,
      trial_index,
      time_elapsed,
      participant_id,
      block
    )
  
  return(participant_data)
}

data_path <- list.files(
  path = "/Users/annacao/Documents/github/testolin2020/data", 
  pattern = "*.csv", 
  full.names = TRUE
)

all_data <- map_dfr(data_path, function(file) {
  data <- load_participant_data(file)
  name <- unique(data$participant_id)
  message("Loaded participant: ", name)
  data
})
  

#### Data exclusion / filtering

#### Prepare data for analysis - create columns etc.
  
# participant correctness column 
compute_correctness <- function(merged_data) {
  merged_data %>%
    mutate(
      correct_response = case_when(
        greater_side == "left" ~ "f",
        greater_side == "right" ~ "j",
        TRUE ~ NA_character_
      ),
      correct = (response == correct_response)
    )
}

  merged <- all_data %>%
    left_join(pairs_complete, by = c("pair_order")) %>%
    compute_correctness() %>%
    
    group_by(participant_id, block) %>%
    mutate(trial_in_block = row_number()) %>%
    ungroup()
  
  qc <- merged %>%
    group_by(participant_id) %>%
    summarise(
      n_trials   = n(),
      n_missing  = sum(is.na(left_numerosity)),
      mean_correct = mean(correct, na.rm = TRUE),
      .groups = "drop"
    )
  
  qc %>% 
    rowwise() %>%
    mutate(
      msg = sprintf(
        "Participant %s: %d trials, %d missing merges, %.1f%% correct",
        participant_id, n_trials, n_missing, 100 * mean_correct
      )
    ) %>%
    pull(msg) %>%
    walk(message)



```

### Confirmatory analysis

I will fit the GLMM model to the dataframe. Specifically, fixed effects will be log-ratios of numerosity, size, spacing, random effects will be Participant-specific differences in slope. From the resulting model I will compare the effect sizes of numerosity, size, spacing, and individual difference, to determine which/whether dimension is the main predictor of performance (or which dimension dominates approximate number discrimination).  

```{r}

summary_stats <- merged %>%
    group_by(participant_id) %>%
    summarise(
      n_trials = n(),
      accuracy = mean(correct, na.rm = TRUE),
      mean_rt = mean(rt, na.rm = TRUE),
      median_rt = median(rt, na.rm = TRUE),
      .groups = "drop"
    )

#### confirmatory GLM
 merged <- merged %>%
  mutate(
    # 1 = chose right (J), 0 = chose left (F)
    chose_right = if_else(response == "j", 1L, 0L)
  )

fit_glm_per_participant <- function(data) {
  data %>%
    group_by(participant_id) %>%
    group_modify(~{
      model <- glm(
        chose_right ~ log_num_ratio + log_size_ratio + log_spacing_ratio,
        data = .x,
        family = binomial(link = "probit")
      )
      tidy(model) %>%
        mutate(
          participant_id = unique(.x$participant_id),
          n_trials = nrow(.x),
          pseudo_r2 = 1 - model$deviance / model$null.deviance
        )
    }) %>%
    ungroup()
}

glm_results <- fit_glm_per_participant(merged)


```


*Side-by-side graph with original graph is ideal here*

```{r}
beta_wide <- glm_results %>%
  filter(term %in% c("log_num_ratio", "log_size_ratio", "log_spacing_ratio")) %>%
  select(participant_id, term, estimate) %>%
  pivot_wider(
    names_from  = term,
    values_from = estimate
  ) %>%
  rename(
    betaNum     = log_num_ratio,
    betaSize    = log_size_ratio,
    betaSpacing = log_spacing_ratio
  )

beta_long <- beta_wide %>%
  pivot_longer(cols = c(betaNum, betaSize, betaSpacing),
               names_to = "dimension",
               values_to = "beta")


#### scatter plots for beta_numerosity, beta_spacing, beta_size
scatter_theme <- theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text  = element_text(size = 10)
  )

# Panel 1: βNum vs βSize
p_num_size <- ggplot(beta_wide, aes(x = betaSize, y = betaNum)) +
  geom_hline(yintercept = 0, colour = "grey70") +
  geom_vline(xintercept = 0, colour = "grey70") +
  geom_abline(intercept = 0, slope = 1, colour = "grey85") +
  geom_abline(intercept = 0, slope = -1, colour = "grey85") +
  geom_point(shape = 21, colour = "red", fill = NA, size = 2) +
  coord_equal(xlim = c(-6, 6), ylim = c(-6, 6)) +
  labs(
    x = expression(beta[Size]),
    y = expression(beta[Num])
  ) +
  scatter_theme

# Panel 2: βNum vs βSpacing
p_num_spacing <- ggplot(beta_wide, aes(x = betaSpacing, y = betaNum)) +
  geom_hline(yintercept = 0, colour = "grey70") +
  geom_vline(xintercept = 0, colour = "grey70") +
  geom_abline(intercept = 0, slope = 1, colour = "grey85") +
  geom_abline(intercept = 0, slope = -1, colour = "grey85") +
  geom_point(shape = 21, colour = "red", fill = NA, size = 2) +
  coord_equal(xlim = c(-6, 6), ylim = c(-6, 6)) +
  labs(
    x = expression(beta[Spacing]),
    y = expression(beta[Num])
  ) +
  scatter_theme

# Panel 3: βSpacing vs βSize
p_spacing_size <- ggplot(beta_wide, aes(x = betaSize, y = betaSpacing)) +
  geom_hline(yintercept = 0, colour = "grey70") +
  geom_vline(xintercept = 0, colour = "grey70") +
  geom_abline(intercept = 0, slope = 1, colour = "grey85") +
  geom_abline(intercept = 0, slope = -1, colour = "grey85") +
  geom_point(shape = 21, colour = "red", fill = NA, size = 2) +
  coord_equal(xlim = c(-6, 6), ylim = c(-6, 6)) +
  labs(
    x = expression(beta[Size]),
    y = expression(beta[Spacing])
  ) +
  scatter_theme


#### individual GLM psychometric curve
library(dplyr)
library(ggplot2)
library(broom)

add_congruency_flags <- function(df,
                                 size_thresh    = 0.7,  # |log2 ratio| threshold
                                 spacing_thresh = 0.7) {
  df %>%
    mutate(
      size_extreme      = abs(log_size_ratio)    >= size_thresh,
      spacing_extreme   = abs(log_spacing_ratio) >= spacing_thresh
    )
}

merged_flagged <- add_congruency_flags(merged)

plot_psychometric_participant <- function(df, pid,
                                          size_thresh = 0.7,
                                          spacing_thresh = 0.7) {
  # subset to one participant and add flags
  d <- df %>%
    filter(participant_id == pid) %>%
    add_congruency_flags(size_thresh = size_thresh,
                         spacing_thresh = spacing_thresh)
  
  # fit the confirmatory GLM (probit)
  m <- glm(
    chose_right ~ log_num_ratio + log_size_ratio + log_spacing_ratio,
    data   = d,
    family = binomial(link = "probit")
  )
  
  # ---- empirical points: p(ChooseRight) by num ratio & condition ----
  
  # all trials (black circles)
  emp_all <- d %>%
    group_by(log_num_ratio) %>%
    summarise(
      p_right = mean(chose_right),
      n       = n(),
      .groups = "drop"
    )
  
  # SIZE: extreme trials only (red)
  size_ext <- d %>% filter(size_extreme)
  
  emp_size_cong <- size_ext %>%
    filter(congruent_size) %>%
    group_by(log_num_ratio) %>%
    summarise(p_right = mean(chose_right),
              .groups = "drop")
  
  emp_size_incong <- size_ext %>%
    filter(!congruent_size) %>%
    group_by(log_num_ratio) %>%
    summarise(p_right = mean(chose_right),
              .groups = "drop")
  
  # SPACING: extreme trials only (green)
  spacing_ext <- d %>% filter(spacing_extreme)
  
  emp_spacing_cong <- spacing_ext %>%
    filter(congruent_spacing) %>%
    group_by(log_num_ratio) %>%
    summarise(p_right = mean(chose_right),
              .groups = "drop")
  
  emp_spacing_incong <- spacing_ext %>%
    filter(!congruent_spacing) %>%
    group_by(log_num_ratio) %>%
    summarise(p_right = mean(chose_right),
              .groups = "drop")
  
  # ---- predicted curves from GLM ----
  # grid of numerosity ratios
  x_grid <- seq(min(d$log_num_ratio), max(d$log_num_ratio), length.out = 200)
  
  # helper: mean log ratios in each extreme subset for offsets
  mean_size_cong <- mean(emp_size_cong$log_num_ratio * 0 + 
                           (size_ext %>% filter(congruent_size))$log_size_ratio)
  mean_size_incong <- mean(size_ext %>% filter(!congruent_size) %>% pull(log_size_ratio))
  mean_spacing_cong <- mean(spacing_ext %>% filter(congruent_spacing) %>% pull(log_spacing_ratio))
  mean_spacing_incong <- mean(spacing_ext %>% filter(!congruent_spacing) %>% pull(log_spacing_ratio))
  
  # baseline: all trials, non-numerical dimensions set to 0
  grid_all <- data.frame(
    log_num_ratio   = x_grid,
    log_size_ratio  = 0,
    log_spacing_ratio = 0,
    cond = "All trials"
  )
  
  # size congruent / incongruent (vary size, spacing=0)
  grid_size_cong <- data.frame(
    log_num_ratio   = x_grid,
    log_size_ratio  = mean_size_cong,
    log_spacing_ratio = 0,
    cond = "Size congruent"
  )
  grid_size_incong <- data.frame(
    log_num_ratio   = x_grid,
    log_size_ratio  = mean_size_incong,
    log_spacing_ratio = 0,
    cond = "Size incongruent"
  )
  
  # spacing congruent / incongruent (vary spacing, size=0)
  grid_spacing_cong <- data.frame(
    log_num_ratio   = x_grid,
    log_size_ratio  = 0,
    log_spacing_ratio = mean_spacing_cong,
    cond = "Spacing congruent"
  )
  grid_spacing_incong <- data.frame(
    log_num_ratio   = x_grid,
    log_size_ratio  = 0,
    log_spacing_ratio = mean_spacing_incong,
    cond = "Spacing incongruent"
  )
  
  grid_all_curves <- bind_rows(
    grid_all,
    grid_size_cong,
    grid_size_incong,
    grid_spacing_cong,
    grid_spacing_incong
  ) %>%
    mutate(p_hat = predict(m, newdata = ., type = "response"))
  
  # ---- plot ----
  ggplot() +
    # black: all trials (points + solid curve)
    geom_point(data = emp_all,
               aes(x = log_num_ratio, y = p_right),
               shape = 21, size = 2, colour = "black") +
    geom_line(data = grid_all_curves %>% filter(cond == "All trials"),
              aes(x = log_num_ratio, y = p_hat),
              colour = "black") +
    
    # red: size congruent (dashed line + open circles)
    geom_line(data = grid_all_curves %>% filter(cond == "Size congruent"),
              aes(x = log_num_ratio, y = p_hat),
              colour = "red", linetype = "dashed") +
    geom_point(data = emp_size_cong,
               aes(x = log_num_ratio, y = p_right),
               colour = "red", shape = 1, size = 2) +
    
    # red: size incongruent (dotted line + small dots)
    geom_line(data = grid_all_curves %>% filter(cond == "Size incongruent"),
              aes(x = log_num_ratio, y = p_hat),
              colour = "red", linetype = "dotted") +
    geom_point(data = emp_size_incong,
               aes(x = log_num_ratio, y = p_right),
               colour = "red", shape = 16, size = 1.5) +
    
    # green: spacing congruent (dashed line + open circles)
    geom_line(data = grid_all_curves %>% filter(cond == "Spacing congruent"),
              aes(x = log_num_ratio, y = p_hat),
              colour = "green4", linetype = "dashed") +
    geom_point(data = emp_spacing_cong,
               aes(x = log_num_ratio, y = p_right),
               colour = "green4", shape = 1, size = 2) +
    
    # green: spacing incongruent (dotted line + small dots)
    geom_line(data = grid_all_curves %>% filter(cond == "Spacing incongruent"),
              aes(x = log_num_ratio, y = p_hat),
              colour = "green4", linetype = "dotted") +
    geom_point(data = emp_spacing_incong,
               aes(x = log_num_ratio, y = p_right),
               colour = "green4", shape = 16, size = 1.5) +
    
    coord_cartesian(ylim = c(0, 1), xlim = c(-1, 1)) +
    labs(
      x = expression("Numerosity Ratio (log"[2]*")"),
      y = "p(Choose Right)",
      title = paste("Participant", pid)
    ) +
    theme_classic()
}

example_id <- unique(merged_flagged$participant_id)[2]

plot_psychometric_participant(merged_flagged, example_id)

```


### Exploratory analyses

Any follow-up analyses desired (not required).  

```{r}
#### GLMM analysis
install.packages(c("lme4", "broom.mixed", "performance"))
library(lme4)
library(broom.mixed)
library(dplyr)

glmm_model <- glmer(
  chose_right ~ log_num_ratio + log_size_ratio + log_spacing_ratio + (1 | participant_id),
  data = merged,
  family = binomial(link = "probit"),
  control = glmerControl(optimizer = "bobyqa")
)

summary(glmm_model)

glmm_results <- broom.mixed::tidy(glmm_model, effects = "fixed")

glmm_results
```


## Discussion

### Summary of Replication Attempt

Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long.
